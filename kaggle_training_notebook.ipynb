{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14501944,"sourceType":"datasetVersion","datasetId":9262340}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n!pip uninstall -y tensorflow\n!pip install -q \"protobuf==3.20.3\" bitsandbytes transformers diffusers accelerate sentencepiece\n\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\n\ntry:\n    user_secrets = UserSecretsClient()\n    \n    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\n    \n    wandb_api = user_secrets.get_secret(\"WANDB_API_KEY\")\n    wandb.login(key=wandb_api)\n    print(\"‚úÖ Logged in to HuggingFace and WandB\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Auth Warning: {e}\")\n    print(\"Ensure 'HF_TOKEN' and 'WANDB_API_KEY' are in Add-ons -> Secrets\")\n\n!git clone https://github.com/Particle1904/SingleStreamDiT_T5Gemma2\nprint(\"‚úÖ Repo Cloned\")\n\nrepo_dir = \"SingleStreamDiT_T5Gemma2\"\noutput_dir = os.path.join(repo_dir, \"output\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"repo_name = \"SingleStreamDiT_T5Gemma2\"\nif os.path.exists(repo_name):\n    os.chdir(repo_name)\n    print(f\"üìÇ Changed directory to: {os.getcwd()}\")\nelse:\n    print(\"‚ùå Repo folder not found. Did Cell 1 run successfully?\")\n\nif os.path.exists(output_dir):\n    shutil.rmtree(output_dir)\n    print(f\"üßπ Cleaned: Removed existing '{output_dir}' to start fresh.\")\nelse:\n    print(\"‚ÑπÔ∏è No output folder found in repo, starting with clean slate.\")\n\nexpected_path = \"/kaggle/input/oxfordflowers/cached_data\"\nif not os.path.exists(expected_path):\n    print(f\"‚ùå WARNING: Dataset not found at {expected_path}\")\n    print(\"Ensure your dataset slug is 'oxfordflowers' and it contains 'cached_data'.\")\nelse:\n    print(\"‚úÖ Dataset found.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile config.py\nimport os\nimport torch\nfrom accelerate.utils import set_seed\nfrom accelerate import Accelerator, DistributedDataParallelKwargs\n\nddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n_accelerator = Accelerator(log_with=\"wandb\", kwargs_handlers=[ddp_kwargs])\nset_seed(42)\n\nclass Config:\n    is_kaggle = os.path.exists(\"/kaggle/working\")\n    \n    if is_kaggle:\n        output_dir = \"/kaggle/working/output\"\n        cache_dir = \"/kaggle/input/oxfordflowers/cached_data\"\n    else:\n        cache_dir = \"./cached_data\"  \n        output_dir = \"./output\"\n    \n    project_name = \"flowers\"\n    dataset_dir = \"./dataset\"\n    checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n    samples_dir = os.path.join(output_dir, \"samples\")\n    log_dir = os.path.join(output_dir, \"logs\")\n    log_file = os.path.join(log_dir, f\"{project_name}_log.csv\")    \n    target_file = os.path.join(cache_dir, \"39.pt\")        \n    resume_from = None\n    reset_optmizer = True\n        \n    text_embed_dim = 1152\n    in_channels = 16    \n\n    hidden_size = 768\n    num_heads = 12\n    depth = 16\n    refiner_depth = 2\n    max_token_length = 128\n    patch_size = 2\n    rope_base = 10_000\n    \n    vae_id = \"diffusers/FLUX.1-vae\"\n    text_model_id = \"google/t5gemma-2-1b-1b\"\n    \n    target_resolution = 448\n    bucket_alignment = 32\n    vae_scaling_factor = 0.3611\n    vae_downsample_factor= 8\n    dataset_mean = 0.0\n    dataset_std = 1.0\n\n    learning_rate = 2e-4   \n    epochs = 1200\n    batch_size = 24\n    accum_steps = 1\n    loss_type = \"mse\"\n    \n    model_dropout = 0.05\n    weight_decay = 0.05\n    optimizer_warmup = 0.05\n    offset_noise = 0.05\n    text_dropout = 0.15\n    flip_aug = False       \n    \n    shift_val = 1.0        \n    \n    use_self_eval = True\n    start_self_eval_at = 0.90\n    self_eval_lambda = 0.3\n    \n    fal_lambda = 0.05\n    fcl_lambda = 0.05\n    fourier_stack_depth = 2\n    if is_kaggle:\n        dtype = torch.float32\n    else:\n        dtype = torch.bfloat16\n    gradient_checkpointing = True\n    use_ema = True\n    ema_decay = 0.999\n    \n    accelerator = _accelerator\n    device = _accelerator.device\n    load_entire_dataset = True\n    num_workers = 2 if os.name != 'nt' else 0\n    \n    save_every = 100\n    validate_every = 50\n    validate_cfg = 3.00\n    validate_steps = 30 \n    validate_sampler = \"euler\"\n    \n    inference_steps = 50\n    guidance_scale = 3.5\n    sampler = \"rk4\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import builtins\nfrom config import Config\n\nsilencer_code = \"\"\"\nimport builtins\nfrom config import Config\n# If this is a worker GPU, kill the print function\nif not Config.accelerator.is_main_process:\n    builtins.print = lambda *args, **kwargs: None\n\"\"\"\n\nif os.path.exists(\"train.py\"):\n    with open(\"train.py\", \"r\") as f: content = f.read()\n    if \"builtins.print = lambda\" not in content: # Avoid double injection\n        with open(\"train.py\", \"w\") as f: f.write(silencer_code + \"\\n\" + content)\n        print(\"‚úÖ Muted logs on Worker GPU (train.py)\")\n\n!sed -i 's/tqdm(self.files, desc=\"Loading Dataset\")/tqdm(self.files, desc=\"Loading Dataset\", disable=not Config.accelerator.is_main_process)/g' dataset.py\nprint(\"‚úÖ Fixed double progress bar (dataset.py)\")\n\n#!sed -i \"s/if sys.platform.startswith('linux'):/if False: # Disabled for T4 stability/g\" train.py\n#print(\"‚úÖ Disabled torch.compile (Fixes T4/FFT crash)\")\n!sed -i 's/mode=\"max-autotune\"/mode=\"reduce-overhead\"/g' train.py\nprint(\"‚úÖ Set torch.compile to reduce-overhead mode\")\n\nprint(\"‚úÖ Patched config.py: Enabled find_unused_parameters=True\")\n\nprint(\"üöÄ Launching Accelerator...\")\n!accelerate launch --multi_gpu --num_processes=2 --mixed_precision=fp16 train.py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}